{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "\n",
    "seed = 42 # Set the seed value for experiment reproducibility.\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "DATASET_PATH = '.\\\\datasets\\\\UrbanSounds8K\\\\resampled_wav_16000'\n",
    "DATASET_PROCESSED_PATH = '.\\\\datasets\\\\UrbanSounds8K\\\\resampled_wav_16000_processed'\n",
    "folds = FOLD_PATHS = np.array(['fold1','fold2','fold3','fold4','fold5',\n",
    "                               'fold6','fold7','fold8','fold9','fold10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_segments_and_labels(filenames_list, window_size):\n",
    "\n",
    "    '''\n",
    "    Get audio segments of same window_size from each file in filenames_list\n",
    "    Each window has 0.1 overlap and rolls back to start of file if as padding\n",
    "    Labels are extracted from filenames of Urbansounds8K Dataset\n",
    "    \n",
    "    Inputs: <list> filenames_list \n",
    "            <int>  window_size\n",
    "    Outputs: <2D np.array> waves_all ; shape=(n_segments,window_size)\n",
    "             <list>        labels \n",
    "    \n",
    "    '''\n",
    "\n",
    "    ### Private function, generate windows with 0.1 overlap\n",
    "    def _windows(data, window_size):\n",
    "        start = 0\n",
    "        while start < len(data):\n",
    "            yield int(start), int(start + window_size)\n",
    "            start += int(window_size * 0.5)\n",
    "\n",
    "    def _get_label(filename):\n",
    "        label = filename.split('\\\\')[-1].split('-')[1]\n",
    "        return label\n",
    "\n",
    "    waves_all,labels_all, lengths_all=[],[],[]\n",
    "    for file in filenames_list:\n",
    "        label=_get_label(file)\n",
    "        waveform, sr = librosa.load(file, mono=1)\n",
    "        length = len(waveform)\n",
    "        lengths_all.append(length)\n",
    "        # print(np.shape(waveform))\n",
    "\n",
    "        # waves, labels=[],[]\n",
    "        for (start, stop) in _windows(waveform, window_size):\n",
    "            wave = waveform[start:stop]    \n",
    "            if wave.shape[0] < window_size:\n",
    "            # while wave.shape[0] < window_size:\n",
    "                # wave = np.concatenate((wave, waveform[:window_size-wave.shape[0]]))\n",
    "                wave = np.concatenate((wave, np.zeros(window_size-wave.shape[0])))\n",
    "            waves_all.append(wave)\n",
    "            labels_all.append(label)\n",
    "        \n",
    "        # waves_all.append(waves)\n",
    "        # labels_all.append(labels)\n",
    "\n",
    "    return waves_all, labels_all\n",
    "\n",
    "# # Test code\n",
    "WINDOW_SIZE = 16000\n",
    "filenames = glob.glob(os.path.join(DATASET_PATH, FOLD_PATHS[0], \"*.wav\"))\n",
    "wavs, labels = get_audio_segments_and_labels(filenames, WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(wavs)\n",
    "import tensorflow_io as tfio\n",
    "def get_spectrogram(waveform):\n",
    "  waveform = tf.cast(waveform, dtype=tf.float32)\n",
    "\n",
    "  # Convert to spectrogram\n",
    "  spectrogram = tfio.audio.spectrogram(waveform, nfft=512, \n",
    "      # window=512, stride=256)\n",
    "      window=1024, stride=512)\n",
    "#   spectrogram = tf.abs(spectrogram)\n",
    "\n",
    "  spectrogram = tfio.audio.melscale(\n",
    "      spectrogram, rate=16000, mels=64, fmin=0, fmax=6000)\n",
    "      # spectrogram, rate=16000, mels=128, fmin=0, fmax=8000)\n",
    "\n",
    "  spectrogram = tfio.audio.dbscale(\n",
    "      spectrogram, top_db=80)\n",
    "      \n",
    "  return spectrogram\n",
    "\n",
    "for wav in wavs[9000:]:\n",
    "    plt.plot(wav)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesses_and_get_labels(file_list, window_size):\n",
    "    data_list, labels = get_audio_segments_and_labels(file_list, window_size)\n",
    "    # data_list_flat = [item for sublist in data_list for item in sublist]\n",
    "    # labels_flat = [item for sublist in labels for item in sublist]\n",
    "    # return data_list_flat, labels_flat\n",
    "    return data_list, labels\n",
    "\n",
    "# test_list, test_labels = preprocesses_and_get_labels(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
    "import numpy as np\n",
    "\n",
    "augment = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "    # Shift(min_fraction=-0.5, max_fraction=0.5, p=0.5),\n",
    "    # AddBackgroundNoise\n",
    "    # AddShortNoises\n",
    "])\n",
    "\n",
    "# Augment/transform/perturb the audio data\n",
    "def augment_list(wave_array):\n",
    "    augmented_list = []\n",
    "    for wave in wave_array:\n",
    "        augmented_list.append(augment(samples=wave, sample_rate=16000))\n",
    "    return augmented_list\n",
    "    \n",
    "# augmented_test_list = augment_list(np.copy(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(fold_path, dataset_path, datasets_path, window_size):\n",
    "  filenames = glob.glob(os.path.join(dataset_path, fold_path, \"*.wav\"))\n",
    "  # print(np.shape(filenames))\n",
    "\n",
    "  fold_list, fold_labels = preprocesses_and_get_labels(filenames, window_size)\n",
    "  ds = tf.data.Dataset.from_tensor_slices((fold_list, fold_labels))\n",
    "  ds = ds.map(\n",
    "    lambda data, labels: (tf.cast(data, dtype='float32'), int(labels)),\n",
    "  )\n",
    "#   augmented_list = augment_list(np.copy(fold_list))\n",
    "#   augmented_ds = tf.data.Dataset.from_tensor_slices((augmented_list, fold_labels))\n",
    "#   combined_ds = ds.concatenate(augmented_ds)\n",
    "  combined_ds = ds\n",
    "\n",
    "  def dataset_save(dataset, dataset_path):\n",
    "      tf.data.experimental.save(\n",
    "          dataset, dataset_path, compression=None, shard_func=None, checkpoint_args=None\n",
    "      )\n",
    "\n",
    "  dataset_save(combined_ds, datasets_path + '/' + fold_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 16000\n",
    "# Generate the dataset\n",
    "os.system(\"mkdir \"+ DATASET_PROCESSED_PATH)\n",
    "for FOLD_PATH in FOLD_PATHS:\n",
    "    save_dataset(FOLD_PATH, DATASET_PATH, DATASET_PROCESSED_PATH, WINDOW_SIZE)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aef9c008b311f0b7f3d27d4f3907c3c9c136ad861e53efda71f92a04d644c5c8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('usc39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
